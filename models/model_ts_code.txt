
Code for root model, type=TracingAdapter:
class TracingAdapter(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  model : __torch__.detectron2.modeling.meta_arch.rcnn.GeneralizedRCNN
  def forward(self: __torch__.detectron2.export.flatten.TracingAdapter,
    argument_1: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
    _0 = __torch__.detectron2.layers.wrappers.move_device_like
    _1 = self.model.roi_heads
    _2 = self.model.proposal_generator
    _3 = self.model.backbone
    _4 = self.model.pixel_std
    _5 = self.model.pixel_mean
    x = _0(argument_1, _5, )
    t = torch.div(torch.sub(x, _5), _4)
    _6 = ops.prim.NumToTensor(torch.size(t, 1))
    _7 = ops.prim.NumToTensor(torch.size(t, 2))
    image_size = torch.stack([_6, _7])
    max_size, _8 = torch.max(torch.stack([image_size]), 0)
    _9 = torch.div(torch.add(max_size, CONSTANTS.c0), CONSTANTS.c1, rounding_mode="floor")
    max_size0 = torch.mul(_9, CONSTANTS.c1)
    _10 = torch.sub(torch.select(max_size0, 0, -1), torch.select(image_size, 0, 1))
    _11 = int(_10)
    _12 = torch.sub(torch.select(max_size0, 0, -2), torch.select(image_size, 0, 0))
    _13 = torch.constant_pad_nd(t, [0, _11, 0, int(_12)], 0.)
    batched_imgs = torch.unsqueeze_(_13, 0)
    x0 = torch.contiguous(batched_imgs)
    _14, _15, _16, _17, _18, _19, _20, _21, _22, _23, _24, = (_3).forward(x0, )
    _25 = (_2).forward(_14, _15, _16, _17, _18, _19, _20, _21, _22, image_size, )
    _26 = (_1).forward(_14, _25, _15, _16, _23, image_size, _24, )
    _27, _28, _29, _30, = _26
    return (_27, _28, _29, _30, image_size)

--------------------------------------------------------------------------------
Code for .model, type=GeneralizedRCNN:
class GeneralizedRCNN(Module):
  __parameters__ = []
  __buffers__ = ["pixel_mean", "pixel_std", ]
  pixel_mean : Tensor
  pixel_std : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  backbone : __torch__.detectron2.modeling.backbone.fpn.FPN
  proposal_generator : __torch__.detectron2.modeling.proposal_generator.rpn.RPN
  roi_heads : __torch__.detectron2.modeling.roi_heads.roi_heads.StandardROIHeads

--------------------------------------------------------------------------------
Code for .model.backbone, type=FPN:
class FPN(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  fpn_lateral2 : __torch__.detectron2.layers.wrappers.Conv2d
  fpn_output2 : __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d
  fpn_lateral3 : __torch__.detectron2.layers.wrappers.___torch_mangle_1.Conv2d
  fpn_output3 : __torch__.detectron2.layers.wrappers.___torch_mangle_2.Conv2d
  fpn_lateral4 : __torch__.detectron2.layers.wrappers.___torch_mangle_3.Conv2d
  fpn_output4 : __torch__.detectron2.layers.wrappers.___torch_mangle_4.Conv2d
  fpn_lateral5 : __torch__.detectron2.layers.wrappers.___torch_mangle_5.Conv2d
  fpn_output5 : __torch__.detectron2.layers.wrappers.___torch_mangle_6.Conv2d
  top_block : __torch__.detectron2.modeling.backbone.fpn.LastLevelMaxPool
  bottom_up : __torch__.detectron2.modeling.backbone.resnet.ResNet
  def forward(self: __torch__.detectron2.modeling.backbone.fpn.FPN,
    x: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:
    _0 = self.top_block
    _1 = self.fpn_output2
    _2 = self.fpn_lateral2
    _3 = self.fpn_output3
    _4 = self.fpn_lateral3
    _5 = self.fpn_output4
    _6 = self.fpn_lateral4
    _7 = self.fpn_output5
    _8 = self.fpn_lateral5
    _9, _10, _11, _12, = (self.bottom_up).forward(x, )
    _13 = (_8).forward(_9, )
    _14 = (_7).forward(_13, )
    top_down_features = torch.upsample_nearest2d(_13, None, [2., 2.])
    x0 = torch.add((_6).forward(_10, ), top_down_features)
    _15 = (_5).forward(x0, )
    top_down_features0 = torch.upsample_nearest2d(x0, None, [2., 2.])
    x1 = torch.add((_4).forward(_11, ), top_down_features0)
    _16 = (_3).forward(x1, )
    top_down_features1 = torch.upsample_nearest2d(x1, None, [2., 2.])
    x2 = torch.add((_2).forward(_12, ), top_down_features1)
    _17 = ((_1).forward(x2, ), _16, _15, _14, _14, _14, _14, (_0).forward(_14, ), _14, _14, _14)
    return _17

--------------------------------------------------------------------------------
Code for .model.backbone.fpn_lateral2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.bias
    lateral_features = torch._convolution(argument_1, self.weight, _0, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return lateral_features

--------------------------------------------------------------------------------
Code for .model.backbone.fpn_output2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.bias
    feature_map = torch._convolution(x, self.weight, _0, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return feature_map

--------------------------------------------------------------------------------
Code for .model.backbone.fpn_lateral3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_1.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.bias
    lateral_features = torch._convolution(argument_1, self.weight, _0, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return lateral_features

--------------------------------------------------------------------------------
Code for .model.backbone.fpn_output3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_2.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.bias
    feature_map = torch._convolution(x, self.weight, _0, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return feature_map

--------------------------------------------------------------------------------
Code for .model.backbone.fpn_lateral4, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_3.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.bias
    lateral_features = torch._convolution(argument_1, self.weight, _0, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return lateral_features

--------------------------------------------------------------------------------
Code for .model.backbone.fpn_output4, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_4.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.bias
    feature_map = torch._convolution(x, self.weight, _0, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return feature_map

--------------------------------------------------------------------------------
Code for .model.backbone.fpn_lateral5, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_5.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.bias
    x = torch._convolution(argument_1, self.weight, _0, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return x

--------------------------------------------------------------------------------
Code for .model.backbone.fpn_output5, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_6.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.bias
    input = torch._convolution(argument_1, self.weight, _0, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return input

--------------------------------------------------------------------------------
Code for .model.backbone.top_block, type=LastLevelMaxPool:
class LastLevelMaxPool(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.modeling.backbone.fpn.LastLevelMaxPool,
    argument_1: Tensor) -> Tensor:
    feature_map = torch.max_pool2d(argument_1, [1, 1], [2, 2], [0, 0], [1, 1])
    return feature_map

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up, type=ResNet:
class ResNet(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  stem : __torch__.detectron2.modeling.backbone.resnet.BasicStem
  res2 : __torch__.torch.nn.modules.container.Sequential
  res3 : __torch__.torch.nn.modules.container.___torch_mangle_60.Sequential
  res4 : __torch__.torch.nn.modules.container.___torch_mangle_105.Sequential
  res5 : __torch__.torch.nn.modules.container.___torch_mangle_129.Sequential
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.ResNet,
    x: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
    _0 = self.res5
    _1 = self.res4
    _2 = self.res3
    _3 = (self.res2).forward((self.stem).forward(x, ), )
    _4 = (_2).forward(_3, )
    _5 = (_1).forward(_4, )
    return ((_0).forward(_5, ), _5, _4, _3)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stem, type=BasicStem:
class BasicStem(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.BasicStem,
    x: Tensor) -> Tensor:
    input = torch.relu_((self.conv1).forward(x, ))
    x0 = torch.max_pool2d(input, [3, 3], [2, 2], [1, 1], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stem.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [2, 2], [3, 3], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stem.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    x = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return x

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_22.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_29.BottleneckBlock
  def forward(self: __torch__.torch.nn.modules.container.Sequential,
    argument_1: Tensor) -> Tensor:
    _0 = getattr(self, "2")
    _1 = getattr(self, "1")
    _2 = (getattr(self, "0")).forward(argument_1, )
    return (_0).forward((_1).forward(_2, ), )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_9.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_11.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_13.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    _0 = self.shortcut
    _1 = self.conv3
    _2 = self.conv2
    x = torch.relu_((self.conv1).forward(argument_1, ))
    x0 = torch.relu_((_2).forward(x, ))
    out = torch.add_((_1).forward(x0, ), (_0).forward(argument_1, ))
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_8.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_9.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_8.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    shortcut = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return shortcut

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_10.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_11.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_10.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_12.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_13.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_12.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_14.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_14.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_17.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_19.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_21.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_22.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    _0 = self.conv3
    _1 = self.conv2
    x = torch.relu_((self.conv1).forward(argument_1, ))
    x0 = torch.relu_((_1).forward(x, ))
    out = torch.add_((_0).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_16.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_17.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_16.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_18.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_19.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_18.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_20.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_21.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_20.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_24.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_26.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_28.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_29.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    _0 = self.conv3
    _1 = self.conv2
    x = torch.relu_((self.conv1).forward(argument_1, ))
    x0 = torch.relu_((_1).forward(x, ))
    out = torch.add_((_0).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_23.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_24.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_23.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_25.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_26.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_25.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_27.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_28.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_27.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_38.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_45.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_52.BottleneckBlock
  __annotations__["3"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_59.BottleneckBlock
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_60.Sequential,
    argument_1: Tensor) -> Tensor:
    _0 = getattr(self, "3")
    _1 = getattr(self, "2")
    _2 = getattr(self, "1")
    _3 = (getattr(self, "0")).forward(argument_1, )
    _4 = (_0).forward((_1).forward((_2).forward(_3, ), ), )
    return _4

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_31.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_33.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_35.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_37.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_38.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    _0 = self.shortcut
    _1 = self.conv3
    _2 = self.conv2
    x = torch.relu_((self.conv1).forward(argument_1, ))
    x0 = torch.relu_((_2).forward(x, ))
    out = torch.add_((_1).forward(x0, ), (_0).forward(argument_1, ))
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_30.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_31.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_30.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    shortcut = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return shortcut

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_32.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_33.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_32.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_34.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_35.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_34.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_36.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_37.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_36.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_40.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_42.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_44.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_45.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    _0 = self.conv3
    _1 = self.conv2
    x = torch.relu_((self.conv1).forward(argument_1, ))
    x0 = torch.relu_((_1).forward(x, ))
    out = torch.add_((_0).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_39.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_40.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_39.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_41.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_42.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_41.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_43.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_44.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_43.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_47.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_49.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_51.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_52.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    _0 = self.conv3
    _1 = self.conv2
    x = torch.relu_((self.conv1).forward(argument_1, ))
    x0 = torch.relu_((_1).forward(x, ))
    out = torch.add_((_0).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_46.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_47.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_46.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_48.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_49.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_48.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_50.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_51.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_50.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.3, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_54.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_56.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_58.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_59.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    _0 = self.conv3
    _1 = self.conv2
    x = torch.relu_((self.conv1).forward(argument_1, ))
    x0 = torch.relu_((_1).forward(x, ))
    out = torch.add_((_0).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.3.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_53.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_54.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.3.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_53.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.3.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_55.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_56.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.3.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_55.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.3.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_57.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_58.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.3.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_57.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_69.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_76.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_83.BottleneckBlock
  __annotations__["3"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_90.BottleneckBlock
  __annotations__["4"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_97.BottleneckBlock
  __annotations__["5"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_104.BottleneckBlock
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_105.Sequential,
    argument_1: Tensor) -> Tensor:
    _0 = getattr(self, "5")
    _1 = getattr(self, "4")
    _2 = getattr(self, "3")
    _3 = getattr(self, "2")
    _4 = getattr(self, "1")
    _5 = (getattr(self, "0")).forward(argument_1, )
    _6 = (_2).forward((_3).forward((_4).forward(_5, ), ), )
    return (_0).forward((_1).forward(_6, ), )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_62.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_64.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_66.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_68.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_69.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    _0 = self.shortcut
    _1 = self.conv3
    _2 = self.conv2
    x = torch.relu_((self.conv1).forward(argument_1, ))
    x0 = torch.relu_((_2).forward(x, ))
    out = torch.add_((_1).forward(x0, ), (_0).forward(argument_1, ))
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_61.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_62.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_61.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    shortcut = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return shortcut

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_63.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_64.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_63.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_65.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_66.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_65.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_67.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_68.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_67.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_71.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_73.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_75.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_76.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    _0 = self.conv3
    _1 = self.conv2
    x = torch.relu_((self.conv1).forward(argument_1, ))
    x0 = torch.relu_((_1).forward(x, ))
    out = torch.add_((_0).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_70.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_71.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_70.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_72.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_73.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_72.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_74.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_75.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_74.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_78.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_80.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_82.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_83.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    _0 = self.conv3
    _1 = self.conv2
    x = torch.relu_((self.conv1).forward(argument_1, ))
    x0 = torch.relu_((_1).forward(x, ))
    out = torch.add_((_0).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_77.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_78.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_77.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_79.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_80.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_79.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_81.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_82.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_81.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.3, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_85.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_87.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_89.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_90.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    _0 = self.conv3
    _1 = self.conv2
    x = torch.relu_((self.conv1).forward(argument_1, ))
    x0 = torch.relu_((_1).forward(x, ))
    out = torch.add_((_0).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.3.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_84.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_85.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.3.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_84.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.3.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_86.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_87.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.3.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_86.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.3.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_88.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_89.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.3.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_88.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.4, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_92.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_94.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_96.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_97.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    _0 = self.conv3
    _1 = self.conv2
    x = torch.relu_((self.conv1).forward(argument_1, ))
    x0 = torch.relu_((_1).forward(x, ))
    out = torch.add_((_0).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.4.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_91.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_92.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.4.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_91.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.4.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_93.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_94.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.4.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_93.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.4.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_95.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_96.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.4.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_95.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.5, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_99.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_101.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_103.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_104.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    _0 = self.conv3
    _1 = self.conv2
    x = torch.relu_((self.conv1).forward(argument_1, ))
    x0 = torch.relu_((_1).forward(x, ))
    out = torch.add_((_0).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.5.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_98.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_99.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.5.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_98.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.5.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_100.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_101.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.5.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_100.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.5.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_102.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_103.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.5.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_102.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_114.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_121.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_128.BottleneckBlock
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_129.Sequential,
    argument_1: Tensor) -> Tensor:
    _0 = getattr(self, "2")
    _1 = getattr(self, "1")
    _2 = (getattr(self, "0")).forward(argument_1, )
    return (_0).forward((_1).forward(_2, ), )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_107.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_109.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_111.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_113.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_114.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    _0 = self.shortcut
    _1 = self.conv3
    _2 = self.conv2
    x = torch.relu_((self.conv1).forward(argument_1, ))
    x0 = torch.relu_((_2).forward(x, ))
    out = torch.add_((_1).forward(x0, ), (_0).forward(argument_1, ))
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_106.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_107.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_106.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    shortcut = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return shortcut

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_108.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_109.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_108.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_110.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_111.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_110.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_112.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_113.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_112.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_116.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_118.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_120.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_121.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    _0 = self.conv3
    _1 = self.conv2
    x = torch.relu_((self.conv1).forward(argument_1, ))
    x0 = torch.relu_((_1).forward(x, ))
    out = torch.add_((_0).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_115.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_116.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_115.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_117.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_118.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_117.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_119.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_120.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_119.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_123.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_125.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_127.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_128.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    _0 = self.conv3
    _1 = self.conv2
    x = torch.relu_((self.conv1).forward(argument_1, ))
    x0 = torch.relu_((_1).forward(x, ))
    out = torch.add_((_0).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_122.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_123.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(argument_1, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_122.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_124.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_125.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_124.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_126.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_127.Conv2d,
    x: Tensor) -> Tensor:
    _0 = self.norm
    input = torch._convolution(x, self.weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_126.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    _0 = self.running_var
    _1 = self.running_mean
    _2 = self.bias
    out = torch.batch_norm(input, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.proposal_generator, type=RPN:
class RPN(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  rpn_head : __torch__.detectron2.modeling.proposal_generator.rpn.StandardRPNHead
  anchor_generator : __torch__.detectron2.modeling.anchor_generator.DefaultAnchorGenerator
  def forward(self: __torch__.detectron2.modeling.proposal_generator.rpn.RPN,
    argument_1: Tensor,
    argument_2: Tensor,
    argument_3: Tensor,
    argument_4: Tensor,
    argument_5: Tensor,
    argument_6: Tensor,
    argument_7: Tensor,
    argument_8: Tensor,
    argument_9: Tensor,
    image_size: Tensor) -> Tensor:
    _0 = __torch__.detectron2.layers.wrappers.move_device_like
    _1 = __torch__.detectron2.layers.wrappers.move_device_like
    _2 = __torch__.detectron2.layers.wrappers.move_device_like
    _3 = __torch__.detectron2.layers.wrappers.move_device_like
    _4 = __torch__.detectron2.layers.wrappers.move_device_like
    _5 = __torch__.detectron2.layers.wrappers.move_device_like
    _6 = __torch__.torchvision.ops.boxes._batched_nms_coordinate_trick
    _7 = self.rpn_head
    _8 = (self.anchor_generator).forward(argument_1, argument_2, argument_3, argument_4, argument_5, argument_6, argument_7, argument_8, )
    _9, _10, _11, _12, _13, = _8
    _14 = (_7).forward(argument_1, argument_2, argument_3, argument_9, argument_8, )
    _15, _16, _17, _18, _19, _20, _21, _22, _23, _24, = _14
    logits_i = torch.flatten(torch.permute(_15, [0, 2, 3, 1]), 1)
    logits_i0 = torch.flatten(torch.permute(_16, [0, 2, 3, 1]), 1)
    logits_i1 = torch.flatten(torch.permute(_17, [0, 2, 3, 1]), 1)
    logits_i2 = torch.flatten(torch.permute(_18, [0, 2, 3, 1]), 1)
    logits_i3 = torch.flatten(torch.permute(_19, [0, 2, 3, 1]), 1)
    _25 = ops.prim.NumToTensor(torch.size(_20, 0))
    _26 = int(_25)
    _27 = ops.prim.NumToTensor(torch.size(_20, 2))
    _28 = int(_27)
    _29 = ops.prim.NumToTensor(torch.size(_20, 3))
    _30 = torch.view(_20, [_26, -1, 4, _28, int(_29)])
    pred_anchor_deltas_i = torch.flatten(torch.permute(_30, [0, 3, 4, 1, 2]), 1, -2)
    _31 = ops.prim.NumToTensor(torch.size(_21, 0))
    _32 = int(_31)
    _33 = ops.prim.NumToTensor(torch.size(_21, 2))
    _34 = int(_33)
    _35 = ops.prim.NumToTensor(torch.size(_21, 3))
    _36 = torch.view(_21, [_32, -1, 4, _34, int(_35)])
    pred_anchor_deltas_i0 = torch.flatten(torch.permute(_36, [0, 3, 4, 1, 2]), 1, -2)
    _37 = ops.prim.NumToTensor(torch.size(_22, 0))
    _38 = int(_37)
    _39 = ops.prim.NumToTensor(torch.size(_22, 2))
    _40 = int(_39)
    _41 = ops.prim.NumToTensor(torch.size(_22, 3))
    _42 = torch.view(_22, [_38, -1, 4, _40, int(_41)])
    pred_anchor_deltas_i1 = torch.flatten(torch.permute(_42, [0, 3, 4, 1, 2]), 1, -2)
    _43 = ops.prim.NumToTensor(torch.size(_23, 0))
    _44 = int(_43)
    _45 = ops.prim.NumToTensor(torch.size(_23, 2))
    _46 = int(_45)
    _47 = ops.prim.NumToTensor(torch.size(_23, 3))
    _48 = torch.view(_23, [_44, -1, 4, _46, int(_47)])
    pred_anchor_deltas_i2 = torch.flatten(torch.permute(_48, [0, 3, 4, 1, 2]), 1, -2)
    _49 = ops.prim.NumToTensor(torch.size(_24, 0))
    _50 = int(_49)
    _51 = ops.prim.NumToTensor(torch.size(_24, 2))
    _52 = int(_51)
    _53 = ops.prim.NumToTensor(torch.size(_24, 3))
    _54 = torch.view(_24, [_50, -1, 4, _52, int(_53)])
    pred_anchor_deltas_i3 = torch.flatten(torch.permute(_54, [0, 3, 4, 1, 2]), 1, -2)
    N = ops.prim.NumToTensor(torch.size(pred_anchor_deltas_i, 0))
    _55 = int(N)
    _56 = int(N)
    _57 = int(N)
    _58 = int(N)
    _59 = int(N)
    _60 = int(N)
    _61 = int(N)
    _62 = int(N)
    _63 = int(N)
    _64 = int(N)
    B = ops.prim.NumToTensor(torch.size(_9, 1))
    _65 = int(B)
    _66 = int(B)
    deltas = torch.reshape(pred_anchor_deltas_i, [-1, int(B)])
    _67 = torch.expand(torch.unsqueeze(_9, 0), [_64, -1, -1])
    boxes = torch.reshape(_67, [-1, _66])
    deltas0 = torch.to(deltas, 6)
    boxes0 = torch.to(boxes, 6)
    _68 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    _69 = torch.select(_68, 1, 2)
    _70 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    widths = torch.sub(_69, torch.select(_70, 1, 0))
    _71 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    _72 = torch.select(_71, 1, 3)
    _73 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    heights = torch.sub(_72, torch.select(_73, 1, 1))
    _74 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    ctr_x = torch.add(torch.select(_74, 1, 0), torch.mul(widths, CONSTANTS.c0))
    _75 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    ctr_y = torch.add(torch.select(_75, 1, 1), torch.mul(heights, CONSTANTS.c0))
    _76 = torch.slice(deltas0, 0, 0, 9223372036854775807)
    _77 = torch.slice(_76, 1, 0, 9223372036854775807, 4)
    dx = torch.div(_77, CONSTANTS.c1)
    _78 = torch.slice(deltas0, 0, 0, 9223372036854775807)
    _79 = torch.slice(_78, 1, 1, 9223372036854775807, 4)
    dy = torch.div(_79, CONSTANTS.c1)
    _80 = torch.slice(deltas0, 0, 0, 9223372036854775807)
    _81 = torch.slice(_80, 1, 2, 9223372036854775807, 4)
    dw = torch.div(_81, CONSTANTS.c1)
    _82 = torch.slice(deltas0, 0, 0, 9223372036854775807)
    _83 = torch.slice(_82, 1, 3, 9223372036854775807, 4)
    dh = torch.div(_83, CONSTANTS.c1)
    dw0 = torch.clamp(dw, None, 4.1351665567423561)
    dh0 = torch.clamp(dh, None, 4.1351665567423561)
    _84 = torch.slice(widths, 0, 0, 9223372036854775807)
    _85 = torch.mul(dx, torch.unsqueeze(_84, 1))
    _86 = torch.slice(ctr_x, 0, 0, 9223372036854775807)
    pred_ctr_x = torch.add(_85, torch.unsqueeze(_86, 1))
    _87 = torch.slice(heights, 0, 0, 9223372036854775807)
    _88 = torch.mul(dy, torch.unsqueeze(_87, 1))
    _89 = torch.slice(ctr_y, 0, 0, 9223372036854775807)
    pred_ctr_y = torch.add(_88, torch.unsqueeze(_89, 1))
    _90 = torch.exp(dw0)
    _91 = torch.slice(widths, 0, 0, 9223372036854775807)
    pred_w = torch.mul(_90, torch.unsqueeze(_91, 1))
    _92 = torch.exp(dh0)
    _93 = torch.slice(heights, 0, 0, 9223372036854775807)
    pred_h = torch.mul(_92, torch.unsqueeze(_93, 1))
    x1 = torch.sub(pred_ctr_x, torch.mul(pred_w, CONSTANTS.c0))
    y1 = torch.sub(pred_ctr_y, torch.mul(pred_h, CONSTANTS.c0))
    x2 = torch.add(pred_ctr_x, torch.mul(pred_w, CONSTANTS.c0))
    y2 = torch.add(pred_ctr_y, torch.mul(pred_h, CONSTANTS.c0))
    pred_boxes = torch.stack([x1, y1, x2, y2], -1)
    _94 = ops.prim.NumToTensor(torch.size(deltas0, 0))
    _95 = int(_94)
    _96 = ops.prim.NumToTensor(torch.size(deltas0, 1))
    proposals_i = torch.reshape(pred_boxes, [_95, int(_96)])
    proposals_i0 = torch.view(proposals_i, [_63, -1, _65])
    B0 = ops.prim.NumToTensor(torch.size(_10, 1))
    _97 = int(B0)
    _98 = int(B0)
    deltas1 = torch.reshape(pred_anchor_deltas_i0, [-1, int(B0)])
    _99 = torch.expand(torch.unsqueeze(_10, 0), [_62, -1, -1])
    boxes1 = torch.reshape(_99, [-1, _98])
    deltas2 = torch.to(deltas1, 6)
    boxes2 = torch.to(boxes1, 6)
    _100 = torch.slice(boxes2, 0, 0, 9223372036854775807)
    _101 = torch.select(_100, 1, 2)
    _102 = torch.slice(boxes2, 0, 0, 9223372036854775807)
    widths0 = torch.sub(_101, torch.select(_102, 1, 0))
    _103 = torch.slice(boxes2, 0, 0, 9223372036854775807)
    _104 = torch.select(_103, 1, 3)
    _105 = torch.slice(boxes2, 0, 0, 9223372036854775807)
    heights0 = torch.sub(_104, torch.select(_105, 1, 1))
    _106 = torch.slice(boxes2, 0, 0, 9223372036854775807)
    ctr_x0 = torch.add(torch.select(_106, 1, 0), torch.mul(widths0, CONSTANTS.c0))
    _107 = torch.slice(boxes2, 0, 0, 9223372036854775807)
    ctr_y0 = torch.add(torch.select(_107, 1, 1), torch.mul(heights0, CONSTANTS.c0))
    _108 = torch.slice(deltas2, 0, 0, 9223372036854775807)
    _109 = torch.slice(_108, 1, 0, 9223372036854775807, 4)
    dx0 = torch.div(_109, CONSTANTS.c1)
    _110 = torch.slice(deltas2, 0, 0, 9223372036854775807)
    _111 = torch.slice(_110, 1, 1, 9223372036854775807, 4)
    dy0 = torch.div(_111, CONSTANTS.c1)
    _112 = torch.slice(deltas2, 0, 0, 9223372036854775807)
    _113 = torch.slice(_112, 1, 2, 9223372036854775807, 4)
    dw1 = torch.div(_113, CONSTANTS.c1)
    _114 = torch.slice(deltas2, 0, 0, 9223372036854775807)
    _115 = torch.slice(_114, 1, 3, 9223372036854775807, 4)
    dh1 = torch.div(_115, CONSTANTS.c1)
    dw2 = torch.clamp(dw1, None, 4.1351665567423561)
    dh2 = torch.clamp(dh1, None, 4.1351665567423561)
    _116 = torch.slice(widths0, 0, 0, 9223372036854775807)
    _117 = torch.mul(dx0, torch.unsqueeze(_116, 1))
    _118 = torch.slice(ctr_x0, 0, 0, 9223372036854775807)
    pred_ctr_x0 = torch.add(_117, torch.unsqueeze(_118, 1))
    _119 = torch.slice(heights0, 0, 0, 9223372036854775807)
    _120 = torch.mul(dy0, torch.unsqueeze(_119, 1))
    _121 = torch.slice(ctr_y0, 0, 0, 9223372036854775807)
    pred_ctr_y0 = torch.add(_120, torch.unsqueeze(_121, 1))
    _122 = torch.exp(dw2)
    _123 = torch.slice(widths0, 0, 0, 9223372036854775807)
    pred_w0 = torch.mul(_122, torch.unsqueeze(_123, 1))
    _124 = torch.exp(dh2)
    _125 = torch.slice(heights0, 0, 0, 9223372036854775807)
    pred_h0 = torch.mul(_124, torch.unsqueeze(_125, 1))
    x10 = torch.sub(pred_ctr_x0, torch.mul(pred_w0, CONSTANTS.c0))
    y10 = torch.sub(pred_ctr_y0, torch.mul(pred_h0, CONSTANTS.c0))
    x20 = torch.add(pred_ctr_x0, torch.mul(pred_w0, CONSTANTS.c0))
    y20 = torch.add(pred_ctr_y0, torch.mul(pred_h0, CONSTANTS.c0))
    pred_boxes0 = torch.stack([x10, y10, x20, y20], -1)
    _126 = ops.prim.NumToTensor(torch.size(deltas2, 0))
    _127 = int(_126)
    _128 = ops.prim.NumToTensor(torch.size(deltas2, 1))
    proposals_i1 = torch.reshape(pred_boxes0, [_127, int(_128)])
    proposals_i2 = torch.view(proposals_i1, [_61, -1, _97])
    B1 = ops.prim.NumToTensor(torch.size(_11, 1))
    _129 = int(B1)
    _130 = int(B1)
    deltas3 = torch.reshape(pred_anchor_deltas_i1, [-1, int(B1)])
    _131 = torch.expand(torch.unsqueeze(_11, 0), [_60, -1, -1])
    boxes3 = torch.reshape(_131, [-1, _130])
    deltas4 = torch.to(deltas3, 6)
    boxes4 = torch.to(boxes3, 6)
    _132 = torch.slice(boxes4, 0, 0, 9223372036854775807)
    _133 = torch.select(_132, 1, 2)
    _134 = torch.slice(boxes4, 0, 0, 9223372036854775807)
    widths1 = torch.sub(_133, torch.select(_134, 1, 0))
    _135 = torch.slice(boxes4, 0, 0, 9223372036854775807)
    _136 = torch.select(_135, 1, 3)
    _137 = torch.slice(boxes4, 0, 0, 9223372036854775807)
    heights1 = torch.sub(_136, torch.select(_137, 1, 1))
    _138 = torch.slice(boxes4, 0, 0, 9223372036854775807)
    ctr_x1 = torch.add(torch.select(_138, 1, 0), torch.mul(widths1, CONSTANTS.c0))
    _139 = torch.slice(boxes4, 0, 0, 9223372036854775807)
    ctr_y1 = torch.add(torch.select(_139, 1, 1), torch.mul(heights1, CONSTANTS.c0))
    _140 = torch.slice(deltas4, 0, 0, 9223372036854775807)
    _141 = torch.slice(_140, 1, 0, 9223372036854775807, 4)
    dx1 = torch.div(_141, CONSTANTS.c1)
    _142 = torch.slice(deltas4, 0, 0, 9223372036854775807)
    _143 = torch.slice(_142, 1, 1, 9223372036854775807, 4)
    dy1 = torch.div(_143, CONSTANTS.c1)
    _144 = torch.slice(deltas4, 0, 0, 9223372036854775807)
    _145 = torch.slice(_144, 1, 2, 9223372036854775807, 4)
    dw3 = torch.div(_145, CONSTANTS.c1)
    _146 = torch.slice(deltas4, 0, 0, 9223372036854775807)
    _147 = torch.slice(_146, 1, 3, 9223372036854775807, 4)
    dh3 = torch.div(_147, CONSTANTS.c1)
    dw4 = torch.clamp(dw3, None, 4.1351665567423561)
    dh4 = torch.clamp(dh3, None, 4.1351665567423561)
    _148 = torch.slice(widths1, 0, 0, 9223372036854775807)
    _149 = torch.mul(dx1, torch.unsqueeze(_148, 1))
    _150 = torch.slice(ctr_x1, 0, 0, 9223372036854775807)
    pred_ctr_x1 = torch.add(_149, torch.unsqueeze(_150, 1))
    _151 = torch.slice(heights1, 0, 0, 9223372036854775807)
    _152 = torch.mul(dy1, torch.unsqueeze(_151, 1))
    _153 = torch.slice(ctr_y1, 0, 0, 9223372036854775807)
    pred_ctr_y1 = torch.add(_152, torch.unsqueeze(_153, 1))
    _154 = torch.exp(dw4)
    _155 = torch.slice(widths1, 0, 0, 9223372036854775807)
    pred_w1 = torch.mul(_154, torch.unsqueeze(_155, 1))
    _156 = torch.exp(dh4)
    _157 = torch.slice(heights1, 0, 0, 9223372036854775807)
    pred_h1 = torch.mul(_156, torch.unsqueeze(_157, 1))
    x11 = torch.sub(pred_ctr_x1, torch.mul(pred_w1, CONSTANTS.c0))
    y11 = torch.sub(pred_ctr_y1, torch.mul(pred_h1, CONSTANTS.c0))
    x21 = torch.add(pred_ctr_x1, torch.mul(pred_w1, CONSTANTS.c0))
    y21 = torch.add(pred_ctr_y1, torch.mul(pred_h1, CONSTANTS.c0))
    pred_boxes1 = torch.stack([x11, y11, x21, y21], -1)
    _158 = ops.prim.NumToTensor(torch.size(deltas4, 0))
    _159 = int(_158)
    _160 = ops.prim.NumToTensor(torch.size(deltas4, 1))
    proposals_i3 = torch.reshape(pred_boxes1, [_159, int(_160)])
    proposals_i4 = torch.view(proposals_i3, [_59, -1, _129])
    B2 = ops.prim.NumToTensor(torch.size(_12, 1))
    _161 = int(B2)
    _162 = int(B2)
    deltas5 = torch.reshape(pred_anchor_deltas_i2, [-1, int(B2)])
    _163 = torch.expand(torch.unsqueeze(_12, 0), [_58, -1, -1])
    boxes5 = torch.reshape(_163, [-1, _162])
    deltas6 = torch.to(deltas5, 6)
    boxes6 = torch.to(boxes5, 6)
    _164 = torch.slice(boxes6, 0, 0, 9223372036854775807)
    _165 = torch.select(_164, 1, 2)
    _166 = torch.slice(boxes6, 0, 0, 9223372036854775807)
    widths2 = torch.sub(_165, torch.select(_166, 1, 0))
    _167 = torch.slice(boxes6, 0, 0, 9223372036854775807)
    _168 = torch.select(_167, 1, 3)
    _169 = torch.slice(boxes6, 0, 0, 9223372036854775807)
    heights2 = torch.sub(_168, torch.select(_169, 1, 1))
    _170 = torch.slice(boxes6, 0, 0, 9223372036854775807)
    ctr_x2 = torch.add(torch.select(_170, 1, 0), torch.mul(widths2, CONSTANTS.c0))
    _171 = torch.slice(boxes6, 0, 0, 9223372036854775807)
    ctr_y2 = torch.add(torch.select(_171, 1, 1), torch.mul(heights2, CONSTANTS.c0))
    _172 = torch.slice(deltas6, 0, 0, 9223372036854775807)
    _173 = torch.slice(_172, 1, 0, 9223372036854775807, 4)
    dx2 = torch.div(_173, CONSTANTS.c1)
    _174 = torch.slice(deltas6, 0, 0, 9223372036854775807)
    _175 = torch.slice(_174, 1, 1, 9223372036854775807, 4)
    dy2 = torch.div(_175, CONSTANTS.c1)
    _176 = torch.slice(deltas6, 0, 0, 9223372036854775807)
    _177 = torch.slice(_176, 1, 2, 9223372036854775807, 4)
    dw5 = torch.div(_177, CONSTANTS.c1)
    _178 = torch.slice(deltas6, 0, 0, 9223372036854775807)
    _179 = torch.slice(_178, 1, 3, 9223372036854775807, 4)
    dh5 = torch.div(_179, CONSTANTS.c1)
    dw6 = torch.clamp(dw5, None, 4.1351665567423561)
    dh6 = torch.clamp(dh5, None, 4.1351665567423561)
    _180 = torch.slice(widths2, 0, 0, 9223372036854775807)
    _181 = torch.mul(dx2, torch.unsqueeze(_180, 1))
    _182 = torch.slice(ctr_x2, 0, 0, 9223372036854775807)
    pred_ctr_x2 = torch.add(_181, torch.unsqueeze(_182, 1))
    _183 = torch.slice(heights2, 0, 0, 9223372036854775807)
    _184 = torch.mul(dy2, torch.unsqueeze(_183, 1))
    _185 = torch.slice(ctr_y2, 0, 0, 9223372036854775807)
    pred_ctr_y2 = torch.add(_184, torch.unsqueeze(_185, 1))
    _186 = torch.exp(dw6)
    _187 = torch.slice(widths2, 0, 0, 9223372036854775807)
    pred_w2 = torch.mul(_186, torch.unsqueeze(_187, 1))
    _188 = torch.exp(dh6)
    _189 = torch.slice(heights2, 0, 0, 9223372036854775807)
    pred_h2 = torch.mul(_188, torch.unsqueeze(_189, 1))
    x12 = torch.sub(pred_ctr_x2, torch.mul(pred_w2, CONSTANTS.c0))
    y12 = torch.sub(pred_ctr_y2, torch.mul(pred_h2, CONSTANTS.c0))
    x22 = torch.add(pred_ctr_x2, torch.mul(pred_w2, CONSTANTS.c0))
    y22 = torch.add(pred_ctr_y2, torch.mul(pred_h2, CONSTANTS.c0))
    pred_boxes2 = torch.stack([x12, y12, x22, y22], -1)
    _190 = ops.prim.NumToTensor(torch.size(deltas6, 0))
    _191 = int(_190)
    _192 = ops.prim.NumToTensor(torch.size(deltas6, 1))
    proposals_i5 = torch.reshape(pred_boxes2, [_191, int(_192)])
    proposals_i6 = torch.view(proposals_i5, [_57, -1, _161])
    B3 = ops.prim.NumToTensor(torch.size(_13, 1))
    _193 = int(B3)
    _194 = int(B3)
    deltas7 = torch.reshape(pred_anchor_deltas_i3, [-1, int(B3)])
    _195 = torch.expand(torch.unsqueeze(_13, 0), [_56, -1, -1])
    boxes7 = torch.reshape(_195, [-1, _194])
    deltas8 = torch.to(deltas7, 6)
    boxes8 = torch.to(boxes7, 6)
    _196 = torch.slice(boxes8, 0, 0, 9223372036854775807)
    _197 = torch.select(_196, 1, 2)
    _198 = torch.slice(boxes8, 0, 0, 9223372036854775807)
    widths3 = torch.sub(_197, torch.select(_198, 1, 0))
    _199 = torch.slice(boxes8, 0, 0, 9223372036854775807)
    _200 = torch.select(_199, 1, 3)
    _201 = torch.slice(boxes8, 0, 0, 9223372036854775807)
    heights3 = torch.sub(_200, torch.select(_201, 1, 1))
    _202 = torch.slice(boxes8, 0, 0, 9223372036854775807)
    ctr_x3 = torch.add(torch.select(_202, 1, 0), torch.mul(widths3, CONSTANTS.c0))
    _203 = torch.slice(boxes8, 0, 0, 9223372036854775807)
    ctr_y3 = torch.add(torch.select(_203, 1, 1), torch.mul(heights3, CONSTANTS.c0))
    _204 = torch.slice(deltas8, 0, 0, 9223372036854775807)
    _205 = torch.slice(_204, 1, 0, 9223372036854775807, 4)
    dx3 = torch.div(_205, CONSTANTS.c1)
    _206 = torch.slice(deltas8, 0, 0, 9223372036854775807)
    _207 = torch.slice(_206, 1, 1, 9223372036854775807, 4)
    dy3 = torch.div(_207, CONSTANTS.c1)
    _208 = torch.slice(deltas8, 0, 0, 9223372036854775807)
    _209 = torch.slice(_208, 1, 2, 9223372036854775807, 4)
    dw7 = torch.div(_209, CONSTANTS.c1)
    _210 = torch.slice(deltas8, 0, 0, 9223372036854775807)
    _211 = torch.slice(_210, 1, 3, 9223372036854775807, 4)
    dh7 = torch.div(_211, CONSTANTS.c1)
    dw8 = torch.clamp(dw7, None, 4.1351665567423561)
    dh8 = torch.clamp(dh7, None, 4.1351665567423561)
    _212 = torch.slice(widths3, 0, 0, 9223372036854775807)
    _213 = torch.mul(dx3, torch.unsqueeze(_212, 1))
    _214 = torch.slice(ctr_x3, 0, 0, 9223372036854775807)
    pred_ctr_x3 = torch.add(_213, torch.unsqueeze(_214, 1))
    _215 = torch.slice(heights3, 0, 0, 9223372036854775807)
    _216 = torch.mul(dy3, torch.unsqueeze(_215, 1))
    _217 = torch.slice(ctr_y3, 0, 0, 9223372036854775807)
    pred_ctr_y3 = torch.add(_216, torch.unsqueeze(_217, 1))
    _218 = torch.exp(dw8)
    _219 = torch.slice(widths3, 0, 0, 9223372036854775807)
    pred_w3 = torch.mul(_218, torch.unsqueeze(_219, 1))
    _220 = torch.exp(dh8)
    _221 = torch.slice(heights3, 0, 0, 9223372036854775807)
    pred_h3 = torch.mul(_220, torch.unsqueeze(_221, 1))
    x13 = torch.sub(pred_ctr_x3, torch.mul(pred_w3, CONSTANTS.c0))
    y13 = torch.sub(pred_ctr_y3, torch.mul(pred_h3, CONSTANTS.c0))
    x23 = torch.add(pred_ctr_x3, torch.mul(pred_w3, CONSTANTS.c0))
    y23 = torch.add(pred_ctr_y3, torch.mul(pred_h3, CONSTANTS.c0))
    pred_boxes3 = torch.stack([x13, y13, x23, y23], -1)
    _222 = ops.prim.NumToTensor(torch.size(deltas8, 0))
    _223 = int(_222)
    _224 = ops.prim.NumToTensor(torch.size(deltas8, 1))
    proposals_i7 = torch.reshape(pred_boxes3, [_223, int(_224)])
    proposals_i8 = torch.view(proposals_i7, [_55, -1, _193])
    _225 = torch.arange(1, dtype=None, layout=0, device=torch.device("cpu"), pin_memory=False)
    batch_idx = _0(_225, proposals_i0, )
    Hi_Wi_A = ops.prim.NumToTensor(torch.size(logits_i, 1))
    num_proposals_i = torch.clamp(Hi_Wi_A, None, 1000)
    _226 = int(num_proposals_i)
    _227, topk_idx = torch.topk(logits_i, int(num_proposals_i), 1)
    _228 = torch.slice(batch_idx, 0, 0, 9223372036854775807)
    _229 = torch.to(torch.unsqueeze(_228, 1), dtype=4, layout=0, device=torch.device("cpu"))
    topk_idx0 = torch.to(topk_idx, dtype=4, layout=0, device=torch.device("cpu"))
    _230 = annotate(List[Optional[Tensor]], [_229, topk_idx0])
    _231 = torch.index(proposals_i0, _230)
    _232 = torch.full([_226], 0, dtype=4, layout=0, device=torch.device("cpu"), pin_memory=False)
    _233 = _1(_232, proposals_i0, )
    Hi_Wi_A0 = ops.prim.NumToTensor(torch.size(logits_i0, 1))
    num_proposals_i0 = torch.clamp(Hi_Wi_A0, None, 1000)
    _234 = int(num_proposals_i0)
    _235, topk_idx1 = torch.topk(logits_i0, int(num_proposals_i0), 1)
    _236 = torch.slice(batch_idx, 0, 0, 9223372036854775807)
    _237 = torch.to(torch.unsqueeze(_236, 1), dtype=4, layout=0, device=torch.device("cpu"))
    topk_idx2 = torch.to(topk_idx1, dtype=4, layout=0, device=torch.device("cpu"))
    _238 = annotate(List[Optional[Tensor]], [_237, topk_idx2])
    _239 = torch.index(proposals_i2, _238)
    _240 = torch.full([_234], 1, dtype=4, layout=0, device=torch.device("cpu"), pin_memory=False)
    _241 = _2(_240, proposals_i0, )
    Hi_Wi_A1 = ops.prim.NumToTensor(torch.size(logits_i1, 1))
    num_proposals_i1 = torch.clamp(Hi_Wi_A1, None, 1000)
    _242 = int(num_proposals_i1)
    _243, topk_idx3 = torch.topk(logits_i1, int(num_proposals_i1), 1)
    _244 = torch.slice(batch_idx, 0, 0, 9223372036854775807)
    _245 = torch.to(torch.unsqueeze(_244, 1), dtype=4, layout=0, device=torch.device("cpu"))
    topk_idx4 = torch.to(topk_idx3, dtype=4, layout=0, device=torch.device("cpu"))
    _246 = annotate(List[Optional[Tensor]], [_245, topk_idx4])
    _247 = torch.index(proposals_i4, _246)
    _248 = torch.full([_242], 2, dtype=4, layout=0, device=torch.device("cpu"), pin_memory=False)
    _249 = _3(_248, proposals_i0, )
    Hi_Wi_A2 = ops.prim.NumToTensor(torch.size(logits_i2, 1))
    num_proposals_i2 = torch.clamp(Hi_Wi_A2, None, 1000)
    _250 = int(num_proposals_i2)
    _251, topk_idx5 = torch.topk(logits_i2, int(num_proposals_i2), 1)
    _252 = torch.slice(batch_idx, 0, 0, 9223372036854775807)
    _253 = torch.to(torch.unsqueeze(_252, 1), dtype=4, layout=0, device=torch.device("cpu"))
    topk_idx6 = torch.to(topk_idx5, dtype=4, layout=0, device=torch.device("cpu"))
    _254 = annotate(List[Optional[Tensor]], [_253, topk_idx6])
    _255 = torch.index(proposals_i6, _254)
    _256 = torch.full([_250], 3, dtype=4, layout=0, device=torch.device("cpu"), pin_memory=False)
    _257 = _4(_256, proposals_i0, )
    Hi_Wi_A3 = ops.prim.NumToTensor(torch.size(logits_i3, 1))
    num_proposals_i3 = torch.clamp(Hi_Wi_A3, None, 1000)
    _258 = int(num_proposals_i3)
    _259, topk_idx7 = torch.topk(logits_i3, int(num_proposals_i3), 1)
    _260 = torch.slice(batch_idx, 0, 0, 9223372036854775807)
    _261 = torch.to(torch.unsqueeze(_260, 1), dtype=4, layout=0, device=torch.device("cpu"))
    topk_idx8 = torch.to(topk_idx7, dtype=4, layout=0, device=torch.device("cpu"))
    _262 = annotate(List[Optional[Tensor]], [_261, topk_idx8])
    _263 = torch.index(proposals_i8, _262)
    _264 = torch.full([_258], 4, dtype=4, layout=0, device=torch.device("cpu"), pin_memory=False)
    _265 = _5(_264, proposals_i0, )
    topk_scores = torch.cat([_227, _235, _243, _251, _259], 1)
    topk_proposals = torch.cat([_231, _239, _247, _255, _263], 1)
    level_ids = torch.cat([_233, _241, _249, _257, _265])
    tensor = torch.select(topk_proposals, 0, 0)
    tensor0 = torch.to(tensor, 6)
    scores_per_img = torch.select(topk_scores, 0, 0)
    h, w, = torch.unbind(image_size)
    _266 = annotate(number, h)
    _267 = annotate(number, w)
    _268 = annotate(number, h)
    _269 = annotate(number, w)
    _270 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    x14 = torch.clamp(torch.select(_270, 1, 0), 0, _269)
    _271 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    y14 = torch.clamp(torch.select(_271, 1, 1), 0, _268)
    _272 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    x24 = torch.clamp(torch.select(_272, 1, 2), 0, _267)
    _273 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    y24 = torch.clamp(torch.select(_273, 1, 3), 0, _266)
    box = torch.stack([x14, y14, x24, y24], -1)
    _274 = torch.slice(box, 0, 0, 9223372036854775807)
    _275 = torch.select(_274, 1, 2)
    _276 = torch.slice(box, 0, 0, 9223372036854775807)
    widths4 = torch.sub(_275, torch.select(_276, 1, 0))
    _277 = torch.slice(box, 0, 0, 9223372036854775807)
    _278 = torch.select(_277, 1, 3)
    _279 = torch.slice(box, 0, 0, 9223372036854775807)
    heights4 = torch.sub(_278, torch.select(_279, 1, 1))
    item = torch.__and__(torch.gt(widths4, 0.), torch.gt(heights4, 0.))
    item0 = torch.to(item, dtype=11, layout=0, device=torch.device("cpu"))
    _280 = annotate(List[Optional[Tensor]], [item0])
    tensor1 = torch.index(box, _280)
    tensor2 = torch.to(tensor1, 6)
    keep = torch.to(item0, dtype=11, layout=0, device=torch.device("cpu"))
    _281 = annotate(List[Optional[Tensor]], [keep])
    scores_per_img0 = torch.index(scores_per_img, _281)
    keep0 = torch.to(keep, dtype=11, layout=0, device=torch.device("cpu"))
    _282 = annotate(List[Optional[Tensor]], [keep0])
    _283 = torch.index(level_ids, _282)
    _284 = torch.to(tensor2, 6)
    keep1 = _6(_284, scores_per_img0, _283, 0.69999999999999996, )
    item1 = torch.slice(keep1, 0, 0, 1000)
    item2 = torch.to(item1, dtype=4, layout=0, device=torch.device("cpu"))
    _285 = annotate(List[Optional[Tensor]], [item2])
    tensor3 = torch.index(_284, _285)
    return torch.to(tensor3, 6)

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head, type=StandardRPNHead:
class StandardRPNHead(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.detectron2.layers.wrappers.___torch_mangle_130.Conv2d
  objectness_logits : __torch__.torch.nn.modules.conv.Conv2d
  anchor_deltas : __torch__.torch.nn.modules.conv.___torch_mangle_131.Conv2d
  def forward(self: __torch__.detectron2.modeling.proposal_generator.rpn.StandardRPNHead,
    argument_1: Tensor,
    argument_2: Tensor,
    argument_3: Tensor,
    argument_4: Tensor,
    argument_5: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:
    _0 = self.anchor_deltas
    _1 = self.objectness_logits
    _2 = self.conv
    _3 = (_2).forward(argument_1, )
    _4 = (_1).forward(_3, )
    _5 = (_0).forward(_3, )
    _6 = (_2).forward1(argument_2, )
    _7 = (_1).forward1(_6, )
    _8 = (_0).forward1(_6, )
    _9 = (_2).forward2(argument_3, )
    _10 = (_1).forward2(_9, )
    _11 = (_0).forward2(_9, )
    _12 = (_2).forward3(argument_4, )
    _13 = (_1).forward3(_12, )
    _14 = (_0).forward3(_12, )
    _15 = (_2).forward4(argument_5, )
    _16 = (_4, _7, _10, _13, (_1).forward4(_15, ), _5, _8, _11, _14, (_0).forward4(_15, ))
    return _16

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation : __torch__.torch.nn.modules.activation.ReLU
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_130.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.activation
    _1 = self.bias
    input = torch._convolution(argument_1, self.weight, _1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )
  def forward1(self: __torch__.detectron2.layers.wrappers.___torch_mangle_130.Conv2d,
    argument_1: Tensor) -> Tensor:
    _2 = self.activation
    _3 = self.bias
    input = torch._convolution(argument_1, self.weight, _3, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_2).forward1(input, )
  def forward2(self: __torch__.detectron2.layers.wrappers.___torch_mangle_130.Conv2d,
    argument_1: Tensor) -> Tensor:
    _4 = self.activation
    _5 = self.bias
    input = torch._convolution(argument_1, self.weight, _5, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_4).forward2(input, )
  def forward3(self: __torch__.detectron2.layers.wrappers.___torch_mangle_130.Conv2d,
    argument_1: Tensor) -> Tensor:
    _6 = self.activation
    _7 = self.bias
    input = torch._convolution(argument_1, self.weight, _7, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_6).forward3(input, )
  def forward4(self: __torch__.detectron2.layers.wrappers.___torch_mangle_130.Conv2d,
    argument_1: Tensor) -> Tensor:
    _8 = self.activation
    _9 = self.bias
    input = torch._convolution(argument_1, self.weight, _9, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_8).forward4(input, )

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.conv.activation, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)
  def forward1(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)
  def forward2(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)
  def forward3(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)
  def forward4(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.objectness_logits, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.conv.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.bias
    score = torch._convolution(argument_1, self.weight, _0, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return score
  def forward1(self: __torch__.torch.nn.modules.conv.Conv2d,
    argument_1: Tensor) -> Tensor:
    _1 = self.bias
    score = torch._convolution(argument_1, self.weight, _1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return score
  def forward2(self: __torch__.torch.nn.modules.conv.Conv2d,
    argument_1: Tensor) -> Tensor:
    _2 = self.bias
    score = torch._convolution(argument_1, self.weight, _2, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return score
  def forward3(self: __torch__.torch.nn.modules.conv.Conv2d,
    argument_1: Tensor) -> Tensor:
    _3 = self.bias
    score = torch._convolution(argument_1, self.weight, _3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return score
  def forward4(self: __torch__.torch.nn.modules.conv.Conv2d,
    argument_1: Tensor) -> Tensor:
    _4 = self.bias
    score = torch._convolution(argument_1, self.weight, _4, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return score

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.anchor_deltas, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.conv.___torch_mangle_131.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.bias
    x = torch._convolution(argument_1, self.weight, _0, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return x
  def forward1(self: __torch__.torch.nn.modules.conv.___torch_mangle_131.Conv2d,
    argument_1: Tensor) -> Tensor:
    _1 = self.bias
    x = torch._convolution(argument_1, self.weight, _1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return x
  def forward2(self: __torch__.torch.nn.modules.conv.___torch_mangle_131.Conv2d,
    argument_1: Tensor) -> Tensor:
    _2 = self.bias
    x = torch._convolution(argument_1, self.weight, _2, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return x
  def forward3(self: __torch__.torch.nn.modules.conv.___torch_mangle_131.Conv2d,
    argument_1: Tensor) -> Tensor:
    _3 = self.bias
    x = torch._convolution(argument_1, self.weight, _3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return x
  def forward4(self: __torch__.torch.nn.modules.conv.___torch_mangle_131.Conv2d,
    argument_1: Tensor) -> Tensor:
    _4 = self.bias
    x = torch._convolution(argument_1, self.weight, _4, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return x

--------------------------------------------------------------------------------
Code for .model.proposal_generator.anchor_generator, type=DefaultAnchorGenerator:
class DefaultAnchorGenerator(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  cell_anchors : __torch__.detectron2.modeling.anchor_generator.BufferList
  def forward(self: __torch__.detectron2.modeling.anchor_generator.DefaultAnchorGenerator,
    argument_1: Tensor,
    argument_2: Tensor,
    argument_3: Tensor,
    argument_4: Tensor,
    argument_5: Tensor,
    argument_6: Tensor,
    argument_7: Tensor,
    argument_8: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
    _0 = __torch__.detectron2.layers.wrappers.move_device_like
    _1 = __torch__.detectron2.layers.wrappers.move_device_like
    _2 = __torch__.detectron2.layers.wrappers.move_device_like
    _3 = __torch__.detectron2.layers.wrappers.move_device_like
    _4 = __torch__.detectron2.layers.wrappers.move_device_like
    _5 = __torch__.detectron2.layers.wrappers.move_device_like
    _6 = __torch__.detectron2.layers.wrappers.move_device_like
    _7 = __torch__.detectron2.layers.wrappers.move_device_like
    _8 = __torch__.detectron2.layers.wrappers.move_device_like
    _9 = __torch__.detectron2.layers.wrappers.move_device_like
    _10 = getattr(self.cell_anchors, "4")
    _11 = getattr(self.cell_anchors, "3")
    _12 = getattr(self.cell_anchors, "2")
    _13 = getattr(self.cell_anchors, "1")
    _14 = getattr(self.cell_anchors, "0")
    grid_height = ops.prim.NumToTensor(torch.size(argument_1, 2))
    grid_width = ops.prim.NumToTensor(torch.size(argument_1, 3))
    grid_height0 = ops.prim.NumToTensor(torch.size(argument_2, 2))
    grid_width0 = ops.prim.NumToTensor(torch.size(argument_2, 3))
    grid_height1 = ops.prim.NumToTensor(torch.size(argument_3, 2))
    grid_width1 = ops.prim.NumToTensor(torch.size(argument_3, 3))
    grid_height2 = ops.prim.NumToTensor(torch.size(argument_6, 2))
    grid_width2 = ops.prim.NumToTensor(torch.size(argument_7, 3))
    grid_height3 = ops.prim.NumToTensor(torch.size(argument_8, 2))
    grid_width3 = ops.prim.NumToTensor(torch.size(argument_8, 3))
    _15 = annotate(number, torch.mul(grid_width, CONSTANTS.c0))
    _16 = torch.arange(0., _15, 4, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _17 = _0(_16, _14, )
    _18 = torch.mul(grid_height, CONSTANTS.c0)
    _19 = torch.arange(0., annotate(number, _18), 4, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _20 = torch.meshgrid([_1(_19, _14, ), _17])
    shift_y, shift_x, = _20
    shift_x0 = torch.reshape(shift_x, [-1])
    shift_y0 = torch.reshape(shift_y, [-1])
    _21 = [shift_x0, shift_y0, shift_x0, shift_y0]
    shifts = torch.stack(_21, 1)
    _22 = torch.add(torch.view(shifts, [-1, 1, 4]), torch.view(_14, [1, -1, 4]))
    tensor = torch.reshape(_22, [-1, 4])
    _23 = torch.mul(grid_width0, CONSTANTS.c1)
    _24 = torch.arange(0., annotate(number, _23), 8, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _25 = _2(_24, _13, )
    _26 = torch.mul(grid_height0, CONSTANTS.c1)
    _27 = torch.arange(0., annotate(number, _26), 8, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _28 = torch.meshgrid([_3(_27, _13, ), _25])
    shift_y1, shift_x1, = _28
    shift_x2 = torch.reshape(shift_x1, [-1])
    shift_y2 = torch.reshape(shift_y1, [-1])
    _29 = [shift_x2, shift_y2, shift_x2, shift_y2]
    shifts0 = torch.stack(_29, 1)
    _30 = torch.add(torch.view(shifts0, [-1, 1, 4]), torch.view(_13, [1, -1, 4]))
    tensor0 = torch.reshape(_30, [-1, 4])
    _31 = torch.mul(grid_width1, CONSTANTS.c2)
    _32 = torch.arange(0., annotate(number, _31), 16, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _33 = _4(_32, _12, )
    _34 = torch.mul(grid_height1, CONSTANTS.c2)
    _35 = torch.arange(0., annotate(number, _34), 16, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _36 = torch.meshgrid([_5(_35, _12, ), _33])
    shift_y3, shift_x3, = _36
    shift_x4 = torch.reshape(shift_x3, [-1])
    shift_y4 = torch.reshape(shift_y3, [-1])
    _37 = [shift_x4, shift_y4, shift_x4, shift_y4]
    shifts1 = torch.stack(_37, 1)
    _38 = torch.add(torch.view(shifts1, [-1, 1, 4]), torch.view(_12, [1, -1, 4]))
    tensor1 = torch.reshape(_38, [-1, 4])
    _39 = torch.mul(grid_width2, CONSTANTS.c3)
    _40 = torch.arange(0., annotate(number, _39), 32, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _41 = _6(_40, _11, )
    _42 = torch.mul(grid_height2, CONSTANTS.c3)
    _43 = torch.arange(0., annotate(number, _42), 32, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _44 = torch.meshgrid([_7(_43, _11, ), _41])
    shift_y5, shift_x5, = _44
    shift_x6 = torch.reshape(shift_x5, [-1])
    shift_y6 = torch.reshape(shift_y5, [-1])
    _45 = [shift_x6, shift_y6, shift_x6, shift_y6]
    shifts2 = torch.stack(_45, 1)
    _46 = torch.add(torch.view(shifts2, [-1, 1, 4]), torch.view(_11, [1, -1, 4]))
    tensor2 = torch.reshape(_46, [-1, 4])
    _47 = torch.mul(grid_width3, CONSTANTS.c4)
    _48 = torch.arange(0., annotate(number, _47), 64, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _49 = _8(_48, _10, )
    _50 = torch.mul(grid_height3, CONSTANTS.c4)
    _51 = torch.arange(0., annotate(number, _50), 64, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _52 = torch.meshgrid([_9(_51, _10, ), _49])
    shift_y7, shift_x7, = _52
    shift_x8 = torch.reshape(shift_x7, [-1])
    shift_y8 = torch.reshape(shift_y7, [-1])
    _53 = [shift_x8, shift_y8, shift_x8, shift_y8]
    shifts3 = torch.stack(_53, 1)
    _54 = torch.add(torch.view(shifts3, [-1, 1, 4]), torch.view(_10, [1, -1, 4]))
    tensor3 = torch.reshape(_54, [-1, 4])
    tensor4 = torch.to(tensor, 6)
    tensor5 = torch.to(tensor0, 6)
    tensor6 = torch.to(tensor1, 6)
    tensor7 = torch.to(tensor2, 6)
    tensor8 = torch.to(tensor3, 6)
    _55 = (tensor4, tensor5, tensor6, tensor7, tensor8)
    return _55

--------------------------------------------------------------------------------
Code for .model.proposal_generator.anchor_generator.cell_anchors, type=BufferList:
class BufferList(Module):
  __parameters__ = []
  __buffers__ = ["0", "1", "2", "3", "4", ]
  __annotations__ = []
  __annotations__["0"] = Tensor
  __annotations__["1"] = Tensor
  __annotations__["2"] = Tensor
  __annotations__["3"] = Tensor
  __annotations__["4"] = Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]

--------------------------------------------------------------------------------
Code for .model.roi_heads, type=StandardROIHeads:
class StandardROIHeads(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  box_pooler : __torch__.detectron2.modeling.poolers.ROIPooler
  box_head : __torch__.detectron2.modeling.roi_heads.box_head.FastRCNNConvFCHead
  box_predictor : __torch__.detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers
  mask_pooler : __torch__.detectron2.modeling.poolers.___torch_mangle_145.ROIPooler
  mask_head : __torch__.detectron2.modeling.roi_heads.mask_head.MaskRCNNConvUpsampleHead
  def forward(self: __torch__.detectron2.modeling.roi_heads.roi_heads.StandardROIHeads,
    argument_1: Tensor,
    argument_2: Tensor,
    argument_3: Tensor,
    argument_4: Tensor,
    argument_5: Tensor,
    image_size: Tensor,
    argument_7: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
    _0 = __torch__.torchvision.ops.boxes._batched_nms_coordinate_trick
    _1 = self.mask_head
    _2 = self.mask_pooler
    _3 = self.box_predictor
    _4 = self.box_head
    _5 = (self.box_pooler).forward(argument_1, argument_2, argument_3, argument_4, argument_5, )
    _6, _7, = (_3).forward((_4).forward(_5, ), )
    _8 = ops.prim.NumToTensor(torch.size(argument_2, 0))
    _9 = int(_8)
    deltas = torch.to(_6, 6)
    boxes = torch.to(argument_2, 6)
    _10 = torch.slice(boxes, 0, 0, 9223372036854775807)
    _11 = torch.select(_10, 1, 2)
    _12 = torch.slice(boxes, 0, 0, 9223372036854775807)
    widths = torch.sub(_11, torch.select(_12, 1, 0))
    _13 = torch.slice(boxes, 0, 0, 9223372036854775807)
    _14 = torch.select(_13, 1, 3)
    _15 = torch.slice(boxes, 0, 0, 9223372036854775807)
    heights = torch.sub(_14, torch.select(_15, 1, 1))
    _16 = torch.slice(boxes, 0, 0, 9223372036854775807)
    ctr_x = torch.add(torch.select(_16, 1, 0), torch.mul(widths, CONSTANTS.c0))
    _17 = torch.slice(boxes, 0, 0, 9223372036854775807)
    ctr_y = torch.add(torch.select(_17, 1, 1), torch.mul(heights, CONSTANTS.c0))
    _18 = torch.slice(deltas, 0, 0, 9223372036854775807)
    _19 = torch.slice(_18, 1, 0, 9223372036854775807, 4)
    dx = torch.div(_19, CONSTANTS.c1)
    _20 = torch.slice(deltas, 0, 0, 9223372036854775807)
    _21 = torch.slice(_20, 1, 1, 9223372036854775807, 4)
    dy = torch.div(_21, CONSTANTS.c1)
    _22 = torch.slice(deltas, 0, 0, 9223372036854775807)
    _23 = torch.slice(_22, 1, 2, 9223372036854775807, 4)
    dw = torch.div(_23, CONSTANTS.c2)
    _24 = torch.slice(deltas, 0, 0, 9223372036854775807)
    _25 = torch.slice(_24, 1, 3, 9223372036854775807, 4)
    dh = torch.div(_25, CONSTANTS.c2)
    dw0 = torch.clamp(dw, None, 4.1351665567423561)
    dh0 = torch.clamp(dh, None, 4.1351665567423561)
    _26 = torch.slice(widths, 0, 0, 9223372036854775807)
    _27 = torch.mul(dx, torch.unsqueeze(_26, 1))
    _28 = torch.slice(ctr_x, 0, 0, 9223372036854775807)
    pred_ctr_x = torch.add(_27, torch.unsqueeze(_28, 1))
    _29 = torch.slice(heights, 0, 0, 9223372036854775807)
    _30 = torch.mul(dy, torch.unsqueeze(_29, 1))
    _31 = torch.slice(ctr_y, 0, 0, 9223372036854775807)
    pred_ctr_y = torch.add(_30, torch.unsqueeze(_31, 1))
    _32 = torch.exp(dw0)
    _33 = torch.slice(widths, 0, 0, 9223372036854775807)
    pred_w = torch.mul(_32, torch.unsqueeze(_33, 1))
    _34 = torch.exp(dh0)
    _35 = torch.slice(heights, 0, 0, 9223372036854775807)
    pred_h = torch.mul(_34, torch.unsqueeze(_35, 1))
    x1 = torch.sub(pred_ctr_x, torch.mul(pred_w, CONSTANTS.c0))
    y1 = torch.sub(pred_ctr_y, torch.mul(pred_h, CONSTANTS.c0))
    x2 = torch.add(pred_ctr_x, torch.mul(pred_w, CONSTANTS.c0))
    y2 = torch.add(pred_ctr_y, torch.mul(pred_h, CONSTANTS.c0))
    pred_boxes = torch.stack([x1, y1, x2, y2], -1)
    _36 = ops.prim.NumToTensor(torch.size(deltas, 0))
    _37 = int(_36)
    _38 = ops.prim.NumToTensor(torch.size(deltas, 1))
    _39 = torch.reshape(pred_boxes, [_37, int(_38)])
    boxes0, = torch.split_with_sizes(_39, [_9])
    _40 = ops.prim.NumToTensor(torch.size(boxes, 0))
    _41 = int(_40)
    _42 = torch.split_with_sizes(torch.softmax(_7, -1), [_41])
    scores, = _42
    _43 = torch.slice(scores, 0, 0, 9223372036854775807)
    scores0 = torch.slice(_43, 1, 0, -1)
    _44 = ops.prim.NumToTensor(torch.size(boxes0, 1))
    num_bbox_reg_classes = torch.floor_divide(_44, CONSTANTS.c3)
    _45 = int(num_bbox_reg_classes)
    tensor = torch.reshape(boxes0, [-1, 4])
    tensor0 = torch.to(tensor, 6)
    h, w, = torch.unbind(image_size)
    _46 = annotate(number, h)
    _47 = annotate(number, w)
    _48 = annotate(number, h)
    _49 = annotate(number, w)
    _50 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    x10 = torch.clamp(torch.select(_50, 1, 0), 0, _49)
    _51 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    y10 = torch.clamp(torch.select(_51, 1, 1), 0, _48)
    _52 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    x20 = torch.clamp(torch.select(_52, 1, 2), 0, _47)
    _53 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    y20 = torch.clamp(torch.select(_53, 1, 3), 0, _46)
    _54 = torch.stack([x10, y10, x20, y20], -1)
    boxes1 = torch.view(_54, [-1, _45, 4])
    filter_mask = torch.gt(scores0, 0.050000000000000003)
    filter_inds = torch.nonzero(filter_mask)
    _55 = torch.slice(filter_inds, 0, 0, 9223372036854775807)
    _56 = torch.select(_55, 1, 0)
    _57 = torch.select(boxes1, 1, 0)
    _58 = torch.to(_56, dtype=4, layout=0, device=torch.device("cpu"))
    _59 = annotate(List[Optional[Tensor]], [_58])
    boxes2 = torch.index(_57, _59)
    filter_mask0 = torch.to(filter_mask, dtype=11, layout=0, device=torch.device("cpu"))
    _60 = annotate(List[Optional[Tensor]], [filter_mask0])
    scores1 = torch.index(scores0, _60)
    _61 = torch.slice(filter_inds, 0, 0, 9223372036854775807)
    _62 = torch.select(_61, 1, 1)
    boxes3 = torch.to(boxes2, 6)
    keep = _0(boxes3, scores1, _62, 0.5, )
    keep0 = torch.slice(keep, 0, 0, 100)
    keep1 = torch.to(keep0, dtype=4, layout=0, device=torch.device("cpu"))
    _63 = annotate(List[Optional[Tensor]], [keep1])
    tensor1 = torch.index(boxes3, _63)
    keep2 = torch.to(keep1, dtype=4, layout=0, device=torch.device("cpu"))
    _64 = annotate(List[Optional[Tensor]], [keep2])
    _65 = torch.index(scores1, _64)
    keep3 = torch.to(keep2, dtype=4, layout=0, device=torch.device("cpu"))
    _66 = annotate(List[Optional[Tensor]], [keep3])
    filter_inds0 = torch.index(filter_inds, _66)
    tensor2 = torch.to(tensor1, 6)
    _67 = torch.slice(filter_inds0, 0, 0, 9223372036854775807)
    _68 = torch.select(_67, 1, 1)
    _69 = (_2).forward(argument_1, tensor2, argument_3, argument_4, argument_7, )
    _70 = (tensor2, _68, (_1).forward(_69, tensor2, ), _65)
    return _70

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler, type=ROIPooler:
class ROIPooler(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  level_poolers : __torch__.torch.nn.modules.container.ModuleList
  def forward(self: __torch__.detectron2.modeling.poolers.ROIPooler,
    argument_1: Tensor,
    argument_2: Tensor,
    argument_3: Tensor,
    argument_4: Tensor,
    argument_5: Tensor) -> Tensor:
    _0 = __torch__.detectron2.modeling.poolers._convert_boxes_to_pooler_format
    _1 = __torch__.detectron2.modeling.poolers._create_zeros
    _2 = getattr(self.level_poolers, "3")
    _3 = getattr(self.level_poolers, "2")
    _4 = getattr(self.level_poolers, "1")
    _5 = getattr(self.level_poolers, "0")
    _6 = torch.cat([argument_2])
    _7 = ops.prim.NumToTensor(torch.size(argument_2, 0))
    pooler_fmt_boxes = _0(_6, torch.stack([_7]), )
    _8 = torch.slice(argument_2, 0, 0, 9223372036854775807)
    _9 = torch.select(_8, 1, 2)
    _10 = torch.slice(argument_2, 0, 0, 9223372036854775807)
    _11 = torch.sub(_9, torch.select(_10, 1, 0))
    _12 = torch.slice(argument_2, 0, 0, 9223372036854775807)
    _13 = torch.select(_12, 1, 3)
    _14 = torch.slice(argument_2, 0, 0, 9223372036854775807)
    _15 = torch.sub(_13, torch.select(_14, 1, 1))
    box_sizes = torch.sqrt(torch.mul(_11, _15))
    _16 = torch.add(torch.div(box_sizes, CONSTANTS.c0), CONSTANTS.c1)
    _17 = torch.add(torch.log2(_16), CONSTANTS.c2)
    level_assignments = torch.floor(_17)
    level_assignments0 = torch.clamp(level_assignments, 2, 5)
    level_assignments1 = torch.sub(torch.to(level_assignments0, 4), CONSTANTS.c3)
    output = _1(pooler_fmt_boxes, 256, 7, 7, argument_1, )
    x = torch.eq(level_assignments1, 0)
    inds, = torch.nonzero_numpy(x)
    inds0 = torch.to(inds, dtype=4, layout=0, device=torch.device("cpu"))
    _18 = annotate(List[Optional[Tensor]], [inds0])
    rois = torch.index(pooler_fmt_boxes, _18)
    _19 = (_5).forward(rois, argument_1, )
    _20 = annotate(List[Optional[Tensor]], [inds0])
    output0 = torch.index_put_(output, _20, _19)
    x0 = torch.eq(level_assignments1, 1)
    inds1, = torch.nonzero_numpy(x0)
    inds2 = torch.to(inds1, dtype=4, layout=0, device=torch.device("cpu"))
    _21 = annotate(List[Optional[Tensor]], [inds2])
    rois0 = torch.index(pooler_fmt_boxes, _21)
    _22 = (_4).forward(rois0, argument_3, )
    _23 = annotate(List[Optional[Tensor]], [inds2])
    output1 = torch.index_put_(output0, _23, _22)
    x1 = torch.eq(level_assignments1, 2)
    inds3, = torch.nonzero_numpy(x1)
    inds4 = torch.to(inds3, dtype=4, layout=0, device=torch.device("cpu"))
    _24 = annotate(List[Optional[Tensor]], [inds4])
    rois1 = torch.index(pooler_fmt_boxes, _24)
    _25 = (_3).forward(rois1, argument_4, )
    _26 = annotate(List[Optional[Tensor]], [inds4])
    output2 = torch.index_put_(output1, _26, _25)
    x2 = torch.eq(level_assignments1, 3)
    inds5, = torch.nonzero_numpy(x2)
    inds6 = torch.to(inds5, dtype=4, layout=0, device=torch.device("cpu"))
    _27 = annotate(List[Optional[Tensor]], [inds6])
    rois2 = torch.index(pooler_fmt_boxes, _27)
    _28 = (_2).forward(rois2, argument_5, )
    _29 = annotate(List[Optional[Tensor]], [inds6])
    return torch.index_put_(output2, _29, _28)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.layers.roi_align.ROIAlign
  __annotations__["1"] = __torch__.detectron2.layers.roi_align.___torch_mangle_132.ROIAlign
  __annotations__["2"] = __torch__.detectron2.layers.roi_align.___torch_mangle_133.ROIAlign
  __annotations__["3"] = __torch__.detectron2.layers.roi_align.___torch_mangle_134.ROIAlign

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.0, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    _0 = ops.torchvision.roi_align(argument_2, boxes, 0.25, 7, 7, 0, True)
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.1, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_132.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    _0 = ops.torchvision.roi_align(argument_2, boxes, 0.125, 7, 7, 0, True)
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.2, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_133.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    _0 = ops.torchvision.roi_align(argument_2, boxes, 0.0625, 7, 7, 0, True)
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.3, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_134.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    _0 = ops.torchvision.roi_align(argument_2, boxes, 0.03125, 7, 7, 0, True)
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head, type=FastRCNNConvFCHead:
class FastRCNNConvFCHead(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  flatten : __torch__.torch.nn.modules.flatten.Flatten
  fc1 : __torch__.torch.nn.modules.linear.Linear
  fc_relu1 : __torch__.torch.nn.modules.activation.___torch_mangle_135.ReLU
  fc2 : __torch__.torch.nn.modules.linear.___torch_mangle_136.Linear
  fc_relu2 : __torch__.torch.nn.modules.activation.___torch_mangle_137.ReLU
  def forward(self: __torch__.detectron2.modeling.roi_heads.box_head.FastRCNNConvFCHead,
    argument_1: Tensor) -> Tensor:
    _0 = self.fc_relu2
    _1 = self.fc2
    _2 = self.fc_relu1
    _3 = self.fc1
    _4 = (self.flatten).forward(argument_1, )
    _5 = (_1).forward((_2).forward((_3).forward(_4, ), ), )
    return (_0).forward(_5, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.flatten, type=Flatten:
class Flatten(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.flatten.Flatten,
    argument_1: Tensor) -> Tensor:
    return torch.flatten(argument_1, 1)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc1, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.Linear,
    argument_1: Tensor) -> Tensor:
    _0 = self.bias
    input = torch.linear(argument_1, self.weight, _0)
    return input

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc_relu1, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_135.ReLU,
    argument_1: Tensor) -> Tensor:
    return torch.relu(argument_1)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc2, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_136.Linear,
    argument_1: Tensor) -> Tensor:
    _0 = self.bias
    input = torch.linear(argument_1, self.weight, _0)
    return input

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc_relu2, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_137.ReLU,
    argument_1: Tensor) -> Tensor:
    return torch.relu(argument_1)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_predictor, type=FastRCNNOutputLayers:
class FastRCNNOutputLayers(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  cls_score : __torch__.torch.nn.modules.linear.___torch_mangle_138.Linear
  bbox_pred : __torch__.torch.nn.modules.linear.___torch_mangle_139.Linear
  def forward(self: __torch__.detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers,
    argument_1: Tensor) -> Tuple[Tensor, Tensor]:
    _0 = self.bbox_pred
    _1 = (self.cls_score).forward(argument_1, )
    return ((_0).forward(argument_1, ), _1)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_predictor.cls_score, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_138.Linear,
    argument_1: Tensor) -> Tensor:
    _0 = self.bias
    input = torch.linear(argument_1, self.weight, _0)
    return input

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_predictor.bbox_pred, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_139.Linear,
    argument_1: Tensor) -> Tensor:
    _0 = self.bias
    deltas = torch.linear(argument_1, self.weight, _0)
    return deltas

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler, type=ROIPooler:
class ROIPooler(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  level_poolers : __torch__.torch.nn.modules.container.___torch_mangle_144.ModuleList
  def forward(self: __torch__.detectron2.modeling.poolers.___torch_mangle_145.ROIPooler,
    argument_1: Tensor,
    tensor: Tensor,
    argument_3: Tensor,
    argument_4: Tensor,
    argument_5: Tensor) -> Tensor:
    _0 = __torch__.detectron2.modeling.poolers._convert_boxes_to_pooler_format
    _1 = __torch__.detectron2.modeling.poolers._create_zeros
    _2 = getattr(self.level_poolers, "3")
    _3 = getattr(self.level_poolers, "2")
    _4 = getattr(self.level_poolers, "1")
    _5 = getattr(self.level_poolers, "0")
    _6 = torch.cat([tensor])
    _7 = ops.prim.NumToTensor(torch.size(tensor, 0))
    pooler_fmt_boxes = _0(_6, torch.stack([_7]), )
    _8 = torch.slice(tensor, 0, 0, 9223372036854775807)
    _9 = torch.select(_8, 1, 2)
    _10 = torch.slice(tensor, 0, 0, 9223372036854775807)
    _11 = torch.sub(_9, torch.select(_10, 1, 0))
    _12 = torch.slice(tensor, 0, 0, 9223372036854775807)
    _13 = torch.select(_12, 1, 3)
    _14 = torch.slice(tensor, 0, 0, 9223372036854775807)
    _15 = torch.sub(_13, torch.select(_14, 1, 1))
    box_sizes = torch.sqrt(torch.mul(_11, _15))
    _16 = torch.add(torch.div(box_sizes, CONSTANTS.c0), CONSTANTS.c1)
    _17 = torch.add(torch.log2(_16), CONSTANTS.c2)
    level_assignments = torch.floor(_17)
    level_assignments0 = torch.clamp(level_assignments, 2, 5)
    level_assignments1 = torch.sub(torch.to(level_assignments0, 4), CONSTANTS.c3)
    output = _1(pooler_fmt_boxes, 256, 14, 14, argument_1, )
    x = torch.eq(level_assignments1, 0)
    inds, = torch.nonzero_numpy(x)
    inds0 = torch.to(inds, dtype=4, layout=0, device=torch.device("cpu"))
    _18 = annotate(List[Optional[Tensor]], [inds0])
    rois = torch.index(pooler_fmt_boxes, _18)
    _19 = (_5).forward(rois, argument_1, )
    _20 = annotate(List[Optional[Tensor]], [inds0])
    output0 = torch.index_put_(output, _20, _19)
    x0 = torch.eq(level_assignments1, 1)
    inds1, = torch.nonzero_numpy(x0)
    inds2 = torch.to(inds1, dtype=4, layout=0, device=torch.device("cpu"))
    _21 = annotate(List[Optional[Tensor]], [inds2])
    rois0 = torch.index(pooler_fmt_boxes, _21)
    _22 = (_4).forward(rois0, argument_3, )
    _23 = annotate(List[Optional[Tensor]], [inds2])
    output1 = torch.index_put_(output0, _23, _22)
    x1 = torch.eq(level_assignments1, 2)
    inds3, = torch.nonzero_numpy(x1)
    inds4 = torch.to(inds3, dtype=4, layout=0, device=torch.device("cpu"))
    _24 = annotate(List[Optional[Tensor]], [inds4])
    rois1 = torch.index(pooler_fmt_boxes, _24)
    _25 = (_3).forward(rois1, argument_4, )
    _26 = annotate(List[Optional[Tensor]], [inds4])
    output2 = torch.index_put_(output1, _26, _25)
    x2 = torch.eq(level_assignments1, 3)
    inds5, = torch.nonzero_numpy(x2)
    inds6 = torch.to(inds5, dtype=4, layout=0, device=torch.device("cpu"))
    _27 = annotate(List[Optional[Tensor]], [inds6])
    rois2 = torch.index(pooler_fmt_boxes, _27)
    _28 = (_2).forward(rois2, argument_5, )
    _29 = annotate(List[Optional[Tensor]], [inds6])
    return torch.index_put_(output2, _29, _28)

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler.level_poolers, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.layers.roi_align.___torch_mangle_140.ROIAlign
  __annotations__["1"] = __torch__.detectron2.layers.roi_align.___torch_mangle_141.ROIAlign
  __annotations__["2"] = __torch__.detectron2.layers.roi_align.___torch_mangle_142.ROIAlign
  __annotations__["3"] = __torch__.detectron2.layers.roi_align.___torch_mangle_143.ROIAlign

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler.level_poolers.0, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_140.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    _0 = ops.torchvision.roi_align(argument_2, boxes, 0.25, 14, 14, 0, True)
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler.level_poolers.1, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_141.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    _0 = ops.torchvision.roi_align(argument_2, boxes, 0.125, 14, 14, 0, True)
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler.level_poolers.2, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_142.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    _0 = ops.torchvision.roi_align(argument_2, boxes, 0.0625, 14, 14, 0, True)
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler.level_poolers.3, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_143.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    _0 = ops.torchvision.roi_align(argument_2, boxes, 0.03125, 14, 14, 0, True)
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head, type=MaskRCNNConvUpsampleHead:
class MaskRCNNConvUpsampleHead(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mask_fcn1 : __torch__.detectron2.layers.wrappers.___torch_mangle_147.Conv2d
  mask_fcn2 : __torch__.detectron2.layers.wrappers.___torch_mangle_149.Conv2d
  mask_fcn3 : __torch__.detectron2.layers.wrappers.___torch_mangle_151.Conv2d
  mask_fcn4 : __torch__.detectron2.layers.wrappers.___torch_mangle_153.Conv2d
  deconv : __torch__.torch.nn.modules.conv.ConvTranspose2d
  deconv_relu : __torch__.torch.nn.modules.activation.___torch_mangle_154.ReLU
  predictor : __torch__.detectron2.layers.wrappers.___torch_mangle_155.Conv2d
  def forward(self: __torch__.detectron2.modeling.roi_heads.mask_head.MaskRCNNConvUpsampleHead,
    argument_1: Tensor,
    tensor: Tensor) -> Tensor:
    _0 = self.predictor
    _1 = self.deconv_relu
    _2 = self.deconv
    _3 = self.mask_fcn4
    _4 = self.mask_fcn3
    _5 = self.mask_fcn2
    _6 = (self.mask_fcn1).forward(argument_1, )
    _7 = (_3).forward((_4).forward((_5).forward(_6, ), ), )
    _8 = (_0).forward((_1).forward((_2).forward(_7, ), ), )
    _9 = torch.sigmoid(_8)
    _10 = ops.prim.NumToTensor(torch.size(tensor, 0))
    _11 = torch.split_with_sizes(_9, [int(_10)])
    _12, = _11
    return _12

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation : __torch__.torch.nn.modules.activation.___torch_mangle_146.ReLU
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_147.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.activation
    _1 = self.bias
    input = torch._convolution(argument_1, self.weight, _1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn1.activation, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_146.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation : __torch__.torch.nn.modules.activation.___torch_mangle_148.ReLU
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_149.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.activation
    _1 = self.bias
    input = torch._convolution(argument_1, self.weight, _1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn2.activation, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_148.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation : __torch__.torch.nn.modules.activation.___torch_mangle_150.ReLU
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_151.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.activation
    _1 = self.bias
    input = torch._convolution(argument_1, self.weight, _1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn3.activation, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_150.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn4, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation : __torch__.torch.nn.modules.activation.___torch_mangle_152.ReLU
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_153.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.activation
    _1 = self.bias
    input = torch._convolution(argument_1, self.weight, _1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (_0).forward(input, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn4.activation, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_152.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.deconv, type=ConvTranspose2d:
class ConvTranspose2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.conv.ConvTranspose2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.bias
    input = torch._convolution(argument_1, self.weight, _0, [2, 2], [0, 0], [1, 1], True, [0, 0], 1, False, False, True, True)
    return input

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.deconv_relu, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_154.ReLU,
    argument_1: Tensor) -> Tensor:
    return torch.relu(argument_1)

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.predictor, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_155.Conv2d,
    argument_1: Tensor) -> Tensor:
    _0 = self.bias
    pred_mask_logits = torch._convolution(argument_1, self.weight, _0, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return pred_mask_logits

--------------------------------------------------------------------------------